{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "25m26pogrvsnwqnvfb4y",
   "authorId": "4132570847525",
   "authorName": "SATYAM_MODANWAL",
   "authorEmail": "satyam.modanwal@gds.ey.com",
   "sessionId": "3b6b7750-f30e-4af1-9476-632c91c62a89",
   "lastEditTime": 1749639247703
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "c986657f-4ef1-45e7-89c8-bd20a9e44976",
   "metadata": {
    "language": "python",
    "name": "Snowflake_Dataframe"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport time\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark import DataFrame\nfrom snowflake.snowpark.functions import col, current_timestamp, lit\nfrom datetime import datetime\n\n# Start time\nstart_time = datetime.now()\n\n# Session is assumed to be already active in Snowpark Notebook\n# Reading the input table\ninput_df = session.table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.INPUT_3RD_LEVEL_TEST3')\n\n# Simple transformation: filter rows with MRR > 1000\nfiltered_df = input_df.filter(col(\"ARR\") > 3000)\n\n# Optional: Add a processing timestamp column\nresult_df = filtered_df.with_column(\"Processed_At\", current_timestamp())\n\n# Write the output to a new table\nresult_df.write.mode(\"overwrite\").save_as_table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.PATAGONIA_INPUT_TEST_SNOW1')\n\n# End time\nend_time = datetime.now()\n\n# Print duration\nprint(f\"Total time taken: {end_time - start_time}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "470b39f5-fcc9-4941-afa7-78754644215f",
   "metadata": {
    "language": "python",
    "name": "Snowflake_df_filter"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\nimport time\n\n# Start measuring time\nstart_time = time.time()\n\n# Load the table into a Snowflake DataFrame\ninput_df = session.table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.INPUT_3RD_LEVEL_TEST3')\n\n# Filter the data where MRR > 1000\ncache_s_time = time.time()\nfiltered_df = input_df.filter(col(\"ARR\") > 1000)\ncache_e_time = time.time()\nprint(f\"⏱️ Caching df filter snowflake: {cache_e_time - cache_s_time:.2f} seconds\")\n\n# Write the result to the comparison table\ncache_s_time = time.time()\nfiltered_df.write.mode(\"overwrite\").save_as_table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.PATAGONIA_INPUT_TEST_SNOW2')\ncache_e_time = time.time()\nprint(f\"⏱️ Caching df save snowflake: {cache_e_time - cache_s_time:.2f} seconds\")\n    \n\n# Measure the time taken\nend_time = time.time()\nruntime = end_time - start_time\n\nprint(f\"Time taken to process the data: {runtime} seconds\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c95f4c1c-4ff6-4058-bd78-6cf79e0d4610",
   "metadata": {
    "language": "python",
    "name": "Polar_Input1"
   },
   "outputs": [],
   "source": "# Import necessary packages\nimport polars as pl\nimport time\nfrom datetime import datetime\nfrom snowflake.snowpark import Session\n\n# Start time\nstart_time = datetime.now()\n\n# Assuming the Snowflake session is already active\n# Fetch data using Snowpark\ninput_df_snowpark = session.table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.PATAGONIA_INPUT_TEST').to_pandas()\n\n# Convert to Polars DataFrame\ninput_df = pl.DataFrame(input_df_snowpark)\n\n# Simple transformation: filter rows with ARR > 1000\nfiltered_df = input_df.filter(pl.col(\"ARR\") > 1000)\n\n# Optional: Add a processing timestamp column\nfiltered_df = filtered_df.with_columns([pl.lit(datetime.now()).alias(\"Processed_At\")])\n\n# Write the output back to Snowflake using Snowpark\noutput_table = 'SUBSCRIPTION_ACCEL.PYTHON_TESTING.PATAGONIA_INPUT_TEST_SNOW1'\nsession.write_pandas(filtered_df.to_pandas(), output_table, overwrite=True)\n\n# End time\nend_time = datetime.now()\n\n# Print duration\nprint(f\"Total time taken: {end_time - start_time}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eecf4113-6e13-4622-97ca-5b239aca775d",
   "metadata": {
    "language": "python",
    "name": "Polar_Input2"
   },
   "outputs": [],
   "source": "# Import necessary packages\nimport streamlit as st\nimport polars as pl\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom datetime import datetime\n\n# Start time\nstart_time = datetime.now()\n\n# Get the active Snowpark session\nsession = get_active_session()\n\n# Reading the input table directly into a Polars DataFrame\ninput_df = session.table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.INPUT_3RD_LEVEL_TEST3')\n\n# Convert Snowpark DataFrame to Polars DataFrame\npolars_df = pl.from_pandas(input_df.to_pandas())\n\ncache_s_time = time.time()\n\n# Simple transformation: filter rows with MRR > 1000\nfiltered_df = polars_df.filter(pl.col(\"ARR\") > 1000)\ncache_e_time = time.time()\nprint(f\"⏱️ Caching df filter polar: {cache_e_time - cache_s_time:.2f} seconds\")\n\n# Optional: Add a processing timestamp column\nresult_df = filtered_df.with_columns(pl.lit(datetime.now()).alias(\"Processed_At\"))\n\n# Write the output to a new table\n# Note: You may need to convert back to Snowpark DataFrame for writing\ncache_s_time = time.time()\nresult_snowpark_df = session.create_dataframe(result_df.to_pandas())\nresult_snowpark_df.write.mode(\"overwrite\").save_as_table('SUBSCRIPTION_ACCEL.PYTHON_TESTING.PATAGONIA_INPUT_TEST_POLAR')\ncache_e_time = time.time()\nprint(f\"⏱️ Caching df write polar: {cache_e_time - cache_s_time:.2f} seconds\")\n\n# End time\nend_time = datetime.now()\n\n# Print duration\nprint(f\"Total time taken: {end_time - start_time}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9bfbd26-bf32-46ac-91dd-9c73a7651d72",
   "metadata": {
    "language": "python",
    "name": "DuckDB_Polar_Snowflake_Input1"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\nimport time\nimport duckdb\nimport polars as pl\nimport pandas as pd\n\n# ⏱ Measure full pipeline start time\npipeline_start = time.time()\n\n# Snowflake processing\nsf_start = time.time()\ninput_df = session.table(\"INPUT_3RD_LEVEL_TEST3\")\n\n# Filter in Snowflake\ncache_s_time = time.time()\nfiltered_sf_df = input_df.filter(col(\"ARR\") > 1000)\ncache_e_time = time.time()\nprint(f\"⏱️ Time taken for snowflake filtering: {cache_e_time - cache_s_time:.2f} seconds\")\n\n# Save filtered result to Snowflake\ncache_s_time = time.time()\nfiltered_sf_df.write.mode(\"overwrite\").save_as_table(\"PATAGONIA_INPUT_SNOW\")\ncache_e_time = time.time()\n\nsf_end = time.time()\nprint(f\"⏱️ Snowflake filter+write: {sf_end - sf_start:.2f}s, Caching only: {cache_e_time - cache_s_time:.2f}s\")\n\n# Export full Snowflake table to Pandas (once) for reuse\nexport_start = time.time()\npandas_df = input_df.to_pandas()\nexport_end = time.time()\nprint(f\"⏱️ Time to export Snowflake table to Pandas: {export_end - export_start:.2f}s\")\n\n# DuckDB filtering\nduck_start = time.time()\nduck_df = duckdb.query_df(pandas_df, \"df\", \"SELECT * FROM df WHERE ARR > 1000\")\nduck_end = time.time()\nprint(f\"🦆 Time taken for DuckDB filtering: {duck_end - duck_start:.2f}s\")\n\n# Polars filtering\npl_df = pl.from_pandas(pandas_df)\npolars_start = time.time()\nfiltered_polars_df = pl_df.filter(pl.col(\"ARR\") > 1000)\n\npolars_end = time.time()\nprint(f\"🦾 Time taken for Polars filtering: {polars_end - polars_start:.2f}s\")\n\n# Total\npipeline_end = time.time()\nprint(f\"✅ Total pipeline time: {pipeline_end - pipeline_start:.2f}s\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab235624-d34f-448e-8cca-6db06ccd25c3",
   "metadata": {
    "language": "python",
    "name": "DuckDB_Polar_Snowflake_Input2"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\nimport time\nimport duckdb\nimport polars as pl\nimport pandas as pd\n\n# ⏱ Measure full pipeline start time\npipeline_start = time.time()\n\n# Snowflake processing\nsf_start = time.time()\ninput_df = session.table(\"PATAGONIA_INPUT_TEST\")\nsnow_start= time.time()\n# Filter in Snowflake\nfiltered_sf_df = input_df.filter(col(\"ARR\") > 1000)\nsnow_end= time.time()\nprint(f\"⏱️ Snowflake filter: {snow_end - snow_start:.2f}s,\")\n\n# Save filtered result to Snowflake\ncache_s_time = time.time()\nfiltered_sf_df.write.mode(\"overwrite\").save_as_table(\"PATAGONIA_INPUT_SNOW\")\ncache_e_time = time.time()\n\nsf_end = time.time()\nprint(f\"⏱️ Snowflake filter+write: {sf_end - sf_start:.2f}s, Caching only: {cache_e_time - cache_s_time:.2f}s\")\n\n# Export full Snowflake table to Pandas (once) for reuse\nexport_start = time.time()\npandas_df = input_df.to_pandas()\nexport_end = time.time()\nprint(f\"⏱️ Time to export Snowflake table to Pandas: {export_end - export_start:.2f}s\")\n\n# DuckDB filtering\nduck_start = time.time()\nduck_df = duckdb.query_df(pandas_df, \"df\", \"SELECT * FROM df WHERE ARR > 1000\").to_df()\nduck_end = time.time()\nprint(f\"🦆 Time taken for DuckDB filtering: {duck_end - duck_start:.2f}s\")\n\n# Save filtered result to Snowflake (DuckDB)\nduck_save_start = time.time()\nsession.write_pandas(duck_df, \"PATAGONIA_INPUT_DUCK\", overwrite=True)\nduck_save_end = time.time()\nprint(f\"🦆 Time taken to save DuckDB result to Snowflake: {duck_save_end - duck_save_start:.2f}s\")\n\n# Polars filtering\npolars_start = time.time()\npl_df = pl.from_pandas(pandas_df)\nfiltered_polars_df = pl_df.filter(pl.col(\"ARR\") > 1000)\npolars_end = time.time()\nprint(f\"🦾 Time taken for Polars filtering: {polars_end - polars_start:.2f}s\")\n\n# Save filtered result to Snowflake (Polars)\npolars_save_start = time.time()\nsession.write_pandas(filtered_polars_df.to_pandas(), \"PATAGONIA_INPUT_POLAR\", overwrite=True)\npolars_save_end = time.time()\nprint(f\"🦾 Time taken to save Polars result to Snowflake: {polars_save_end - polars_save_start:.2f}s\")\n\n# Total\npipeline_end = time.time()\nprint(f\"✅ Total pipeline time: {pipeline_end - pipeline_start:.2f}s\")\n",
   "execution_count": null
  }
 ]
}